{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:57:33.840467Z",
     "start_time": "2025-05-09T12:57:31.917412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
    "from PIL import Image\n",
    "from transformers.utils.import_utils import is_flash_attn_2_available\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:57:37.384340Z",
     "start_time": "2025-05-09T12:57:33.882824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = ColQwen2.from_pretrained(\n",
    "    \"vidore/colqwen2-v1.0-merged\",\n",
    "    torch_dtype=torch.float32,\n",
    "    attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "    ).eval()"
   ],
   "id": "63f1242e6d79abd1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:57:38.621087Z",
     "start_time": "2025-05-09T12:57:37.394137Z"
    }
   },
   "cell_type": "code",
   "source": "processor = ColQwen2Processor.from_pretrained(\"vidore/colqwen2-v1.0-merged\")",
   "id": "e98fbd16fa289831",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:57:48.930950Z",
     "start_time": "2025-05-09T12:57:48.867315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Your inputs\n",
    "images = [\n",
    "        Image.new(\"RGB\", (8192, 8192), color=\"white\"),\n",
    "        Image.new(\"RGB\", (4096, 4096), color=\"white\"),\n",
    "\n",
    "        ]\n",
    "queries = [\n",
    "        \"Is attention really all you need?\",\n",
    "        \"What is the amount of bananas farmed in Salvador?\",\n",
    "        ]"
   ],
   "id": "99974cdb26a447fd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:57:50.135828Z",
     "start_time": "2025-05-09T12:57:49.114439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process the inputs\n",
    "batch_images = processor.process_images(images, context_prompts=queries)\n",
    "batch_queries = processor.process_queries(queries)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    #     # image_embeddings = model(**batch_images)\n",
    "    query_embeddings = model(**batch_queries)\n",
    "    # query_embeddings2 = model(\n",
    "    #     **batch_queries,\n",
    "    #     pixel_values=(torch.zeros((2, 0, 0), dtype=torch.float32)),\n",
    "    #     image_grid_thw=torch.zeros((2, 3), dtype=torch.long)\n",
    "    #     )\n",
    "    # query_embeddings2 = model(**batch_queries)  #, pixel_values=None, image_grid_thw= None)\n",
    "\n",
    "# scores = processor.score_multi_vector(query_embeddings, image_embeddings)"
   ],
   "id": "547bcbf45154d228",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:58:01.756910Z",
     "start_time": "2025-05-09T12:57:50.325845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create ONNX Runtime session\n",
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(\"onnx_models/colqwen2_combined.onnx\")"
   ],
   "id": "bb5757a234325d73",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:58:17.855357Z",
     "start_time": "2025-05-09T12:58:01.946205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(\"\\nTesting ONNX model with ONNXRuntime...\")\n",
    "\n",
    "import numpy as np\n",
    "from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
    "\n",
    "from converter.colqwen2_model import ColQwen2Wrapper\n",
    "from converter.dummy_inputs import dummy_inputs_for_export_tuple\n",
    "\n",
    "og_model = ColQwen2.from_pretrained(\"vidore/colqwen2-v1.0-merged\")\n",
    "combined_model_wrapper = ColQwen2Wrapper(og_model).eval()\n",
    "dummy_texts = dict(**batch_queries)\n",
    "text_input_ids = dummy_texts[\"input_ids\"]\n",
    "text_attention_mask = dummy_texts[\"attention_mask\"]\n",
    "text_input_dict = {\n",
    "        \"input_ids\": text_input_ids.numpy(),\n",
    "        \"attention_mask\": text_attention_mask.numpy(),\n",
    "        \"pixel_values\": np.zeros((2, 0, 1176), dtype=np.float32),\n",
    "        \"image_grid_thw\": np.zeros((2, 3), dtype=np.int64),\n",
    "        }\n",
    "\n",
    "# Convert PyTorch tensors to NumPy arrays for ONNXRuntime\n",
    "input_dict = {\n",
    "        \"input_ids\": dummy_inputs_for_export_tuple[0].numpy(),\n",
    "        \"attention_mask\": dummy_inputs_for_export_tuple[1].numpy(),\n",
    "        \"pixel_values\": dummy_inputs_for_export_tuple[2].numpy(),\n",
    "        \"image_grid_thw\": dummy_inputs_for_export_tuple[3].numpy(),\n",
    "        }\n",
    "\n",
    "# Run inference with ONNXRuntime\n",
    "ort_outputs = session.run([\"output_embeddings\"], text_input_dict)\n",
    "print(f\"ONNX model output shape: {ort_outputs[0].shape}\")\n",
    "\n",
    "# # # Compare with PyTorch model output\n",
    "# with torch.no_grad():\n",
    "#     pytorch_outputs = combined_model_wrapper(\n",
    "#         **dummy_texts, pixel_values=torch.zeros((2, 0, 1176), dtype=torch.float32),\n",
    "#         image_grid_thw=torch.zeros((2, 3), dtype=torch.int64)\n",
    "#         )\n",
    "#\n",
    "# mean_diff = np.mean(np.abs(ort_outputs[0] - pytorch_outputs.numpy()))\n",
    "# print(f\"Mean absolute difference between PyTorch and ONNX outputs: {mean_diff}\")\n",
    "# print(\"ONNX model test complete.\")"
   ],
   "id": "7431f87c110434f1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1;31m2025-05-09 14:58:17.741615 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running Gather node. Name:'/model/visual/blocks.0/attn/Gather_4' Status Message: indices element out of data bounds, idx=1 must be within the inclusive range [-1,0]\u001B[m\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Gather node. Name:'/model/visual/blocks.0/attn/Gather_4' Status Message: indices element out of data bounds, idx=1 must be within the inclusive range [-1,0]",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mInvalidArgument\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 29\u001B[39m\n\u001B[32m     21\u001B[39m input_dict = {\n\u001B[32m     22\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m\"\u001B[39m: dummy_inputs_for_export_tuple[\u001B[32m0\u001B[39m].numpy(),\n\u001B[32m     23\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mattention_mask\u001B[39m\u001B[33m\"\u001B[39m: dummy_inputs_for_export_tuple[\u001B[32m1\u001B[39m].numpy(),\n\u001B[32m     24\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mpixel_values\u001B[39m\u001B[33m\"\u001B[39m: dummy_inputs_for_export_tuple[\u001B[32m2\u001B[39m].numpy(),\n\u001B[32m     25\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mimage_grid_thw\u001B[39m\u001B[33m\"\u001B[39m: dummy_inputs_for_export_tuple[\u001B[32m3\u001B[39m].numpy(),\n\u001B[32m     26\u001B[39m         }\n\u001B[32m     28\u001B[39m \u001B[38;5;66;03m# Run inference with ONNXRuntime\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m29\u001B[39m ort_outputs = \u001B[43msession\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43moutput_embeddings\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_input_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mONNX model output shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mort_outputs[\u001B[32m0\u001B[39m].shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m# # # Compare with PyTorch model output\u001B[39;00m\n\u001B[32m     33\u001B[39m \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[32m     34\u001B[39m \u001B[38;5;66;03m#     pytorch_outputs = combined_model_wrapper(\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     40\u001B[39m \u001B[38;5;66;03m# print(f\"Mean absolute difference between PyTorch and ONNX outputs: {mean_diff}\")\u001B[39;00m\n\u001B[32m     41\u001B[39m \u001B[38;5;66;03m# print(\"ONNX model test complete.\")\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/onnx-converter-wapNqe8x-py3.13/lib/python3.13/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:270\u001B[39m, in \u001B[36mSession.run\u001B[39m\u001B[34m(self, output_names, input_feed, run_options)\u001B[39m\n\u001B[32m    268\u001B[39m     output_names = [output.name \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._outputs_meta]\n\u001B[32m    269\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m270\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sess\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_feed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    271\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m C.EPFail \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m    272\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._enable_fallback:\n",
      "\u001B[31mInvalidArgument\u001B[39m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Gather node. Name:'/model/visual/blocks.0/attn/Gather_4' Status Message: indices element out of data bounds, idx=1 must be within the inclusive range [-1,0]"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:58:17.866836Z",
     "start_time": "2025-05-09T12:55:02.821452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(input_dict[\"pixel_values\"].shape)\n",
    "print(text_input_dict[\"pixel_values\"].shape)"
   ],
   "id": "896e9e40a0f4fbd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2916, 1176)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'pixel_values'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28mprint\u001B[39m(input_dict[\u001B[33m\"\u001B[39m\u001B[33mpixel_values\u001B[39m\u001B[33m\"\u001B[39m].shape)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtext_input_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpixel_values\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m.shape)\n",
      "\u001B[31mKeyError\u001B[39m: 'pixel_values'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "input_dict[\"image_grid_thw\"].shape",
   "id": "cc326d7796862fc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d0f0ae9c694bd0f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
